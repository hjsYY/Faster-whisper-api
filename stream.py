import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
import time

# åˆå§‹åŒ–æ¨¡åž‹ï¼ˆå¯æ¢ä¸º tiny / small / baseï¼‰
fasterModel = WhisperModel("large", device="cuda", compute_type="float16")


# éŸ³é¢‘å‚æ•°
SAMPLE_RATE = 16000
CHUNK_DURATION = 3  # æ¯æ¬¡å½•åˆ¶ 3 ç§’
CHUNK_SIZE = SAMPLE_RATE * CHUNK_DURATION

print("å¼€å§‹è¯´è¯å§...\n")

try:
    while True:
        # 1. å½•åˆ¶ 3 ç§’éŸ³é¢‘ï¼ˆæ³¨æ„ channels=1 å•å£°é“ï¼‰
        audio_data = sd.rec(CHUNK_SIZE, samplerate=SAMPLE_RATE, channels=1, dtype='float32')
        sd.wait()  # ç­‰å¾…å½•éŸ³ç»“æŸ

        # 2. è½¬æ¢ä¸ºä¸€ç»´ numpy æ•°ç»„
        audio_array = audio_data.flatten()

        # 3. Whisper æ¨¡åž‹è½¬å½• + VAD åŽ»é™éŸ³
        segments, info = fasterModel.transcribe(
            audio_array,
            beam_size=5,
            language="zh",
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=1000)
        )

        # 4. æ‰“å°æ¯ä¸€æ®µç»“æžœ
        for segment in segments:
            print(f"[{segment.start:.1f}s -> {segment.end:.1f}s] {segment.text}")

except KeyboardInterrupt:
    print("\nðŸ›‘ å·²é€€å‡º")
